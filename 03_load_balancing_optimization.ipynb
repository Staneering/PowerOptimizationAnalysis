{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a73c23",
   "metadata": {},
   "source": [
    "# Load Balancing Optimization\n",
    "\n",
    "    ## Load Balancing Optimization\n",
    "    This notebook focuses on optimizing load distribution using Q-learning.\n",
    "    \n",
    "    Steps:\n",
    "    1. Set up Q-learning with states, actions, and rewards.\n",
    "    2. Train the Q-learning model for load balancing.\n",
    "    3. Compare Q-learning with a rule-based optimization method.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9fc144-033a-48be-8434-1d5229f00fe5",
   "metadata": {},
   "source": [
    "The notebook I provided in the previous message sets up the **Q-learning model** and **rule-based optimization**, but it **does not yet fully implement training the Q-learning model** on your dataset for **load balancing** and **compare** the performance of the two models under **real-time conditions** (e.g., load surges or EV charging).\n",
    "\n",
    "Letâ€™s **extend** the notebook to include:\n",
    "\n",
    "1. **Training the Q-learning model on the dataset** for **load balancing**.\n",
    "2. **Comparing Q-learning** with **rule-based optimization** under various conditions.\n",
    "3. **Simulating real-time grid behavior** (load surges, EV charging) and evaluating the performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Updated 3rd Notebook: `03_load_balancing_optimization.ipynb`**\n",
    "\n",
    "#### **Markdown Cell**:\n",
    "\n",
    "```markdown\n",
    "# Load Balancing Optimization\n",
    "\n",
    "In this notebook, we use Q-learning to optimize the distribution of power load across different sub-metering zones (Sub_metering_1, Sub_metering_2, Sub_metering_3).\n",
    "\n",
    "Steps:\n",
    "1. Set up the Q-learning environment with states, actions, and rewards.\n",
    "2. Train the Q-learning model to optimize load distribution.\n",
    "3. Compare the performance of Q-learning with a rule-based optimization approach.\n",
    "4. Test the model under real-time conditions (e.g., load surges, EV charging).\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3cd1eae-e992-487b-aa72-14dc0df309f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable-baselines3[extra]\n",
      "  Downloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting gymnasium<1.2.0,>=0.29.1\n",
      "  Downloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines3[extra]) (3.10.1)\n",
      "Collecting torch<3.0,>=2.3\n",
      "  Downloading torch-2.7.0-cp310-cp310-win_amd64.whl (212.5 MB)\n",
      "Requirement already satisfied: pandas in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines3[extra]) (2.2.3)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines3[extra]) (2.2.4)\n",
      "Collecting pygame\n",
      "  Downloading pygame-2.6.1-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "Requirement already satisfied: psutil in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines3[extra]) (7.0.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines3[extra]) (11.1.0)\n",
      "Collecting tensorboard>=2.9.1\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting ale-py>=0.9.0\n",
      "  Downloading ale_py-0.11.0-cp310-cp310-win_amd64.whl (3.4 MB)\n",
      "Collecting rich\n",
      "  Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Collecting gym_notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ale-py>=0.9.0->stable-baselines3[extra]) (4.12.2)\n",
      "Collecting farama-notifications>=0.0.1\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6\n",
      "  Downloading protobuf-6.30.2-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (57.4.0)\n",
      "Collecting grpcio>=1.48.2\n",
      "  Downloading grpcio-1.71.0-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.6)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.56.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (3.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.19.1)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ibrah\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->stable-baselines3[extra]) (0.4.6)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (PEP 517): started\n",
      "  Building wheel for gym (PEP 517): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827738 sha256=abeb2100ebd5726804db27ef2aa0f109c8611a734abaed02e1531338dd6e9311\n",
      "  Stored in directory: c:\\users\\ibrah\\appdata\\local\\pip\\cache\\wheels\\b9\\22\\6d\\3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
      "Successfully built gym\n",
      "Installing collected packages: mpmath, sympy, networkx, mdurl, fsspec, filelock, farama-notifications, cloudpickle, werkzeug, torch, tensorboard-data-server, protobuf, markdown-it-py, markdown, gymnasium, grpcio, absl-py, tqdm, tensorboard, stable-baselines3, rich, pygame, opencv-python, gym-notices, ale-py, gym\n",
      "Successfully installed absl-py-2.2.2 ale-py-0.11.0 cloudpickle-3.1.1 farama-notifications-0.0.4 filelock-3.18.0 fsspec-2025.3.2 grpcio-1.71.0 gym-0.26.2 gym-notices-0.0.8 gymnasium-1.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 networkx-3.4.2 opencv-python-4.11.0.86 protobuf-6.30.2 pygame-2.6.1 rich-14.0.0 stable-baselines3-2.6.0 sympy-1.14.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 torch-2.7.0 tqdm-4.67.1 werkzeug-3.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Ibrah\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra] gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57cd1ebd-6e4a-49a2-8841-5ca04d5cf76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Ibrah\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install shimmy>=2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20e61eb9-e700-4ed7-828a-971e0a285a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset and preprocess states (as before)\n",
    "file_path = 'preprocessed_power_consumption.csv'\n",
    "df = pd.read_csv(file_path, index_col='Timestamp')\n",
    "\n",
    "def categorize_voltage(voltage):\n",
    "    if voltage < 225:\n",
    "        return 'low_voltage'\n",
    "    elif 225 <= voltage <= 240:\n",
    "        return 'normal_voltage'\n",
    "    else:\n",
    "        return 'high_voltage'\n",
    "\n",
    "def categorize_load(active_power):\n",
    "    if active_power < 3:\n",
    "        return 'low_load'\n",
    "    elif 3 <= active_power <= 6:\n",
    "        return 'medium_load'\n",
    "    else:\n",
    "        return 'high_load'\n",
    "\n",
    "df['Voltage_State'] = df['Voltage'].apply(categorize_voltage)\n",
    "df['Load_State'] = df['Global_active_power'].apply(categorize_load)\n",
    "df['State'] = df['Voltage_State'] + '_' + df['Load_State']\n",
    "\n",
    "# Map states to numerical indices\n",
    "state_labels = df['State'].unique().tolist()\n",
    "state_to_idx = {state: idx for idx, state in enumerate(state_labels)}\n",
    "actions = ['adjust_submetering1', 'adjust_submetering2', 'adjust_submetering3']\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros((len(state_labels), len(actions)))\n",
    "\n",
    "# Simulate state transitions\n",
    "def take_action(state, action):\n",
    "    current_load = df[df['State'] == state]['Global_active_power'].mean()\n",
    "    current_voltage = df[df['State'] == state]['Voltage'].mean()\n",
    "    \n",
    "    # Hypothetical load adjustment\n",
    "    if action == 'adjust_submetering1':\n",
    "        new_load = current_load - 0.5\n",
    "    elif action == 'adjust_submetering2':\n",
    "        new_load = current_load + 0.3\n",
    "    elif action == 'adjust_submetering3':\n",
    "        new_load = current_load - 0.2\n",
    "    \n",
    "    new_voltage = current_voltage - 0.05 * (new_load - current_load)\n",
    "    new_v_state = categorize_voltage(new_voltage)\n",
    "    new_l_state = categorize_load(new_load)\n",
    "    return f\"{new_v_state}_{new_l_state}\"\n",
    "\n",
    "# Reward function with terminal state logic\n",
    "def refined_reward(current_state, next_state):\n",
    "    if 'normal_voltage' in next_state and 'medium_load' in next_state:\n",
    "        return 10  # Terminal state: balanced\n",
    "    elif 'low_voltage' in next_state or 'high_voltage' in next_state:\n",
    "        return -5  # Penalize voltage extremes\n",
    "    elif 'high_load' in next_state:\n",
    "        return -2  # Penalize overload\n",
    "    else:\n",
    "        return 1  # Small reward for progress\n",
    "\n",
    "# Hyperparameters with epsilon decay\n",
    "epsilon = 1.0  # Start with high exploration\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "episodes = 1000\n",
    "max_steps_per_episode = 100  # Terminate after 100 steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666217ac-ab1d-4fcb-8069-6a2ac0ed87bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2, Total Reward: -500, Epsilon: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Training loop with convergence checks\n",
    "for episode in range(episodes):\n",
    "    state = random.choice(state_labels)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Exploration vs. exploitation\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            action_idx = np.argmax(Q[state_to_idx[state]])\n",
    "            action = actions[action_idx]\n",
    "        \n",
    "        next_state = take_action(state, action)\n",
    "        \n",
    "        # Check if next_state exists in Q-table\n",
    "        if next_state not in state_to_idx:\n",
    "            next_state = state  # Treat unknown states as terminal\n",
    "            \n",
    "        reward = refined_reward(state, next_state)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update Q-table\n",
    "        current_idx = state_to_idx[state]\n",
    "        next_idx = state_to_idx[next_state]\n",
    "        Q[current_idx, actions.index(action)] = (1 - learning_rate) * Q[current_idx, actions.index(action)] + \\\n",
    "                                                learning_rate * (reward + discount_factor * np.max(Q[next_idx]))\n",
    "        \n",
    "        # Check terminal state (balanced condition)\n",
    "        if 'normal_voltage_medium_load' in next_state:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "    # Print progress every 100 episodes\n",
    "    if (episode + 1) % 2 == 0:\n",
    "        print(f\"Episode {episode+1}, Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "print(\"Training complete. Q-table:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4b6b149-8670-4c45-97fa-88310d640e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **Code Cell**:\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "file_path = 'preprocessed_power_consumption.csv'  # Adjust path if needed\n",
    "df = pd.read_csv(file_path, index_col='Timestamp')\n",
    "\n",
    "\n",
    "# Define states based on voltage and load\n",
    "def categorize_voltage(voltage):\n",
    "    if voltage < 225:\n",
    "        return 'low_voltage'\n",
    "    elif 225 <= voltage <= 240:\n",
    "        return 'normal_voltage'\n",
    "    else:\n",
    "        return 'high_voltage'\n",
    "\n",
    "def categorize_load(active_power):\n",
    "    if active_power < 3:\n",
    "        return 'low_load'\n",
    "    elif 3 <= active_power <= 6:\n",
    "        return 'medium_load'\n",
    "    else:\n",
    "        return 'high_load'\n",
    "\n",
    "# Create the state column for voltage and load\n",
    "df['Voltage_State'] = df['Voltage'].apply(categorize_voltage)\n",
    "df['Load_State'] = df['Global_active_power'].apply(categorize_load)\n",
    "\n",
    "# Combine both states into one state representation\n",
    "df['State'] = df['Voltage_State'] + '_' + df['Load_State']\n",
    "\n",
    "# Define possible actions (adjusting load for each sub-metering)\n",
    "actions = ['adjust_submetering1', 'adjust_submetering2', 'adjust_submetering3']\n",
    "\n",
    "# Initialize the Q-table (state-action values)\n",
    "Q = np.zeros((len(df['State'].unique()), len(actions)))\n",
    "\n",
    "# Define a reward function for balancing\n",
    "def reward(state, action):\n",
    "    if 'low_voltage' in state or 'high_load' in state:\n",
    "        return -1  # Penalty for low voltage or high load\n",
    "    return 1  # Reward for balancing the load efficiently\n",
    "\n",
    "# **Fine-Tuning the Reward Function**\n",
    "def refined_reward(state, action):\n",
    "    # Penalizing high load or low voltage, and rewarding balancing\n",
    "    if 'low_voltage' in state:\n",
    "        return -5  # Strong penalty for low voltage (voltage sag)\n",
    "    if 'high_load' in state:\n",
    "        return -2  # Penalty for high load (inefficient load distribution)\n",
    "    return 1  # Reward for good load balancing\n",
    "\n",
    "# Hyperparameters for Q-learning\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.2  # Exploration rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da67c3f6-34e2-42de-b8f8-6b85f8f26562",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(n=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d1fd037-2b90-47e8-96d6-fec79ef2a8bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m.columns\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b35960d4-69a9-48ec-9d97-360bd56bf37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global_active_power</th>\n",
       "      <th>Global_reactive_power</th>\n",
       "      <th>Voltage</th>\n",
       "      <th>Global_intensity</th>\n",
       "      <th>Sub_metering_1</th>\n",
       "      <th>Sub_metering_2</th>\n",
       "      <th>Sub_metering_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.065034</td>\n",
       "      <td>0.132497</td>\n",
       "      <td>0.575767</td>\n",
       "      <td>0.066448</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.011623</td>\n",
       "      <td>0.185350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.047805</td>\n",
       "      <td>0.079399</td>\n",
       "      <td>0.056902</td>\n",
       "      <td>0.044123</td>\n",
       "      <td>0.005094</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.242011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.009596</td>\n",
       "      <td>0.033094</td>\n",
       "      <td>0.452989</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.032727</td>\n",
       "      <td>0.078777</td>\n",
       "      <td>0.565104</td>\n",
       "      <td>0.035270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.008065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.048886</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.604847</td>\n",
       "      <td>0.053942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.032258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.099711</td>\n",
       "      <td>0.199281</td>\n",
       "      <td>0.613813</td>\n",
       "      <td>0.097654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015299</td>\n",
       "      <td>0.342407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.155350</td>\n",
       "      <td>0.253237</td>\n",
       "      <td>0.620032</td>\n",
       "      <td>0.149378</td>\n",
       "      <td>0.012749</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Global_active_power  Global_reactive_power    Voltage  \\\n",
       "count            10.000000              10.000000  10.000000   \n",
       "mean              0.065034               0.132497   0.575767   \n",
       "std               0.047805               0.079399   0.056902   \n",
       "min               0.009596               0.033094   0.452989   \n",
       "25%               0.032727               0.078777   0.565104   \n",
       "50%               0.048886               0.114286   0.604847   \n",
       "75%               0.099711               0.199281   0.613813   \n",
       "max               0.155350               0.253237   0.620032   \n",
       "\n",
       "       Global_intensity  Sub_metering_1  Sub_metering_2  Sub_metering_3  \n",
       "count         10.000000       10.000000       10.000000       10.000000  \n",
       "mean           0.066448        0.002411        0.011623        0.185350  \n",
       "std            0.044123        0.005094        0.009354        0.242011  \n",
       "min            0.012448        0.000000        0.000000        0.000000  \n",
       "25%            0.035270        0.000000        0.003125        0.008065  \n",
       "50%            0.053942        0.000000        0.012500        0.032258  \n",
       "75%            0.097654        0.000000        0.015299        0.342407  \n",
       "max            0.149378        0.012749        0.025000        0.580645  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fbb209-41a3-4bd7-ad65-35f4ae9f760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training the Q-learning agent (extended for load balancing)\n",
    "for episode in range(10):  # Number of training episodes\n",
    "    state = random.choice(df['State'].unique())  # Initialize random state\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Exploration vs. exploitation\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)  # Explore a random action\n",
    "        else:\n",
    "            action = actions[np.argmax(Q[df['State'].unique().tolist().index(state)])]  # Exploit learned action\n",
    "\n",
    "        # Get reward for the current state-action pair\n",
    "        r = reward(state, action)\n",
    "        \n",
    "        # Get next state (random for simplicity)\n",
    "        next_state = random.choice(df['State'].unique())\n",
    "        \n",
    "        # Update Q-table using Q-learning formula\n",
    "        Q[df['State'].unique().tolist().index(state), actions.index(action)] = (1 - learning_rate) * Q[df['State'].unique().tolist().index(state), actions.index(action)] + \\\n",
    "                                                                          learning_rate * (r + discount_factor * np.max(Q[df['State'].unique().tolist().index(next_state)]))\n",
    "        \n",
    "        state = next_state  # Transition to the next state\n",
    "    \n",
    "# Display the trained Q-table after training\n",
    "print(\"Trained Q-table:\")\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c26dc1-82b9-4556-bff7-0ee7e216bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class LoadBalancingEnv(gym.Env):\n",
    "    def __init__(self, df):\n",
    "        super(LoadBalancingEnv, self).__init__()\n",
    "\n",
    "        # The action space: Adjust sub-metering load (3 actions)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # The observation space: Features like voltage and load\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(len(df.columns),), dtype=np.float32)\n",
    "\n",
    "        # Store the dataset\n",
    "        self.df = df\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.df.iloc[self.current_step].values  # Return the first observation\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply the action (adjust sub-metering load)\n",
    "        if action == 0:\n",
    "            # Simulate action: Adjust sub-metering 1\n",
    "            pass\n",
    "        elif action == 1:\n",
    "            # Simulate action: Adjust sub-metering 2\n",
    "            pass\n",
    "        elif action == 2:\n",
    "            # Simulate action: Adjust sub-metering 3\n",
    "            pass\n",
    "\n",
    "        # Calculate reward (simplified)\n",
    "        reward = self.calculate_reward(action)\n",
    "\n",
    "        # Increment the step counter\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Check if we're at the end of the dataset\n",
    "        done = self.current_step >= len(self.df) - 1\n",
    "\n",
    "        # Return the next state, reward, and done flag\n",
    "        next_state = self.df.iloc[self.current_step].values\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def calculate_reward(self, action):\n",
    "        # Define reward function (to be refined based on your model's goals)\n",
    "        if action == 0:\n",
    "            return 1  # Reward for adjusting sub-metering 1\n",
    "        elif action == 1:\n",
    "            return -1  # Penalty for adjusting sub-metering 2\n",
    "        else:\n",
    "            return 0  # Neutral reward for adjusting sub-metering 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14382c96-c131-47c4-8b92-2214e18edf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing shimmy installation. You provided an OpenAI Gym environment. Stable-Baselines3 (SB3) has transitioned to using Gymnasium internally. In order to use OpenAI Gym environments with SB3, you need to install shimmy (`pip install 'shimmy>=2.0'`).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:40\u001b[0m, in \u001b[0;36m_patch_env\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshimmy\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'shimmy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m env \u001b[38;5;241m=\u001b[39m LoadBalancingEnv(df)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize the DQN model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)  \u001b[38;5;66;03m# Adjust the number of timesteps for your needs\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:104\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     78\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m[DQNPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m     _init_setup_model: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    103\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No action noise\u001b[39;49;00m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplay_buffer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplay_buffer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43msde_support\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimize_memory_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize_memory_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDiscrete\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupport_multi_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_initial_eps \u001b[38;5;241m=\u001b[39m exploration_initial_eps\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_final_eps \u001b[38;5;241m=\u001b[39m exploration_final_eps\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:110\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, action_noise, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, use_sde_at_warmup, sde_support, supported_action_spaces)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     82\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m[BasePolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     supported_action_spaces: Optional[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtype\u001b[39m[spaces\u001b[38;5;241m.\u001b[39mSpace], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    109\u001b[0m ):\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupport_multi_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msupport_multi_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msupported_action_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size \u001b[38;5;241m=\u001b[39m buffer_size\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:170\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     env \u001b[38;5;241m=\u001b[39m maybe_make_env(env, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m--> 170\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:217\u001b[0m, in \u001b[0;36mBaseAlgorithm._wrap_env\u001b[1;34m(env, verbose, monitor_wrapper)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" \"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03mWrap environment with the appropriate wrappers if needed.\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03mFor instance, to have a vectorized environment\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m:return: The wrapped environment.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env, VecEnv):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# Patch to support gym 0.21/0.26 and gymnasium\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43m_patch_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_wrapped(env, Monitor) \u001b[38;5;129;01mand\u001b[39;00m monitor_wrapper:\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:42\u001b[0m, in \u001b[0;36m_patch_env\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshimmy\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing shimmy installation. You provided an OpenAI Gym environment. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStable-Baselines3 (SB3) has transitioned to using Gymnasium internally. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use OpenAI Gym environments with SB3, you need to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstall shimmy (`pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshimmy>=2.0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     49\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou provided an OpenAI Gym environment. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe strongly recommend transitioning to Gymnasium environments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStable-Baselines3 is automatically wrapping your environments in a compatibility \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer, which could potentially cause issues.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m signature(env\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39mreset)\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Gym 0.26+ env\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing shimmy installation. You provided an OpenAI Gym environment. Stable-Baselines3 (SB3) has transitioned to using Gymnasium internally. In order to use OpenAI Gym environments with SB3, you need to install shimmy (`pip install 'shimmy>=2.0'`)."
     ]
    }
   ],
   "source": [
    "#Step 3: Use Stable-Baselines3 to Train the DQN Model\n",
    "#Now that we have the environment set up, we can use Stable-Baselines3 to train a DQN model on it.\n",
    "\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "# Initialize the environment\n",
    "env = LoadBalancingEnv(df)\n",
    "\n",
    "# Initialize the DQN model\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000)  # Adjust the number of timesteps for your needs\n",
    "\n",
    "# Save the model after training\n",
    "model.save(\"dqn_load_balancing_model\")\n",
    "\n",
    "# Optionally, load the model to continue training or for inference\n",
    "# model = DQN.load(\"dqn_load_balancing_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7aa5b8-3c2c-4c11-b015-426c0410c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "obs = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "for _ in range(len(df)):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"Total reward from model: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be563e03-43ac-4e86-867f-643cb821511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### **Comparing Q-learning with Rule-based Optimization**\n",
    "\n",
    "##Now, weâ€™ll compare the **Q-learning model** with a **rule-based approach**. The rule-based method will adjust sub-metering load based on **simple voltage or load thresholds**.\n",
    "\n",
    "#### **Code for Comparison**:\n",
    "\n",
    "\n",
    "# Rule-based optimization for load balancing (Traditional method)\n",
    "def rule_based_optimization(df):\n",
    "    # Simple rules: If voltage is below 225V, adjust sub-metering load\n",
    "    if df['Voltage'] < 225:\n",
    "        action = 'adjust_submetering1'  # Increase sub-metering 1 load (simulate corrective action)\n",
    "    elif df['Global_active_power'] > 5:\n",
    "        action = 'adjust_submetering2'  # Distribute load to sub-metering 2\n",
    "    else:\n",
    "        action = 'adjust_submetering3'  # Default action\n",
    "    \n",
    "    return action\n",
    "\n",
    "# Function to compare Q-learning with rule-based decisions\n",
    "def compare_models(df, Q, actions):\n",
    "    # Testing with Q-learning\n",
    "    q_learning_action = actions[np.argmax(Q[df['State'].unique().tolist().index(df['State'].iloc[-1])])]\n",
    "    \n",
    "    # Testing with Rule-based\n",
    "    rule_based_action = rule_based_optimization(df)\n",
    "    \n",
    "    print(f\"Q-learning Action: {q_learning_action}, Rule-based Action: {rule_based_action}\")\n",
    "\n",
    "# Example comparison for a sample state\n",
    "df['State'] = 'normal_voltage_medium_load'  # Example state for testing\n",
    "compare_models(df, Q, actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b72593-c4a6-4dda-9eab-4dfd36be7a55",
   "metadata": {},
   "source": [
    "\n",
    "### **Testing the Model Under Real-Time Conditions (e.g., Load Surges, EV Charging)**\n",
    "\n",
    "To simulate **real-time grid behavior**:\n",
    "\n",
    "1. Weâ€™ll simulate a **load surge** (e.g., increase in **Global\\_active\\_power** due to **EV charging**).\n",
    "2. Weâ€™ll observe how **Q-learning** reacts and adjusts the load to **balance** the system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df5c0b-9884-42e2-9676-8fec106264db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **Script for Simulating Real-Time Grid Behavior**:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Simulate multiple conditions for real-time grid behavior\n",
    "def simulate_real_time_conditions(df, time_steps=10):\n",
    "    # Track results over time\n",
    "    voltage_history = []\n",
    "    load_history = []\n",
    "    actions_taken = []\n",
    "    \n",
    "    for i in range(time_steps):\n",
    "        # Simulate Load Surge (e.g., EV charging)\n",
    "        df['Global_active_power'] += np.random.normal(1, 0.5)  # Random surge in active power\n",
    "        \n",
    "        # Simulate renewable energy variability (voltage affected by load)\n",
    "        df['Voltage'] = df['Voltage'] - (df['Global_active_power'] * 0.05)  # Voltage drop from increased load\n",
    "        \n",
    "        # Test the Q-learning model for the adjusted state\n",
    "        current_state = categorize_voltage(df['Voltage'].iloc[-1]) + '_' + categorize_load(df['Global_active_power'].iloc[-1])\n",
    "        action = actions[np.argmax(Q[df['State'].unique().tolist().index(current_state)])]  # Get the best action based on Q-table\n",
    "        \n",
    "        # Track results for analysis\n",
    "        voltage_history.append(df['Voltage'].iloc[-1])\n",
    "        load_history.append(df['Global_active_power'].iloc[-1])\n",
    "        actions_taken.append(action)\n",
    "        \n",
    "        # Simulate other conditions:\n",
    "        # Simulate a voltage spike (random sudden increase in voltage)\n",
    "        if np.random.random() < 0.1:  # 10% chance of voltage spike\n",
    "            df['Voltage'] += np.random.uniform(5, 15)  # Add voltage spike\n",
    "        \n",
    "        # Print results for analysis\n",
    "        print(f\"Time Step {i+1}: Voltage = {df['Voltage'].iloc[-1]:.2f}, Load = {df['Global_active_power'].iloc[-1]:.2f}, Action: {action}\")\n",
    "        \n",
    "    # Plot the simulation results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(voltage_history, label='Voltage (V)', color='b')\n",
    "    plt.title('Voltage Over Time')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Voltage (V)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(load_history, label='Load (kW)', color='r')\n",
    "    plt.title('Load Over Time')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Load (kW)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return actions taken for further analysis\n",
    "    return actions_taken\n",
    "\n",
    "# Simulate 10 time steps of grid behavior (load surge, voltage spike, and real-time adjustments)\n",
    "simulate_real_time_conditions(df, time_steps=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbb1ad-2c9c-4dff-b59c-28c550a0aae7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Next Steps**:\n",
    "\n",
    "1. **Train the Q-learning model** for **load balancing** and test its decisions.\n",
    "2. **Compare the performance** of **Q-learning** and **rule-based optimization** under dynamic grid conditions.\n",
    "3. **Simulate real-time grid behavior** (e.g., **EV charging** and **load surges**) to evaluate how the models perform in **real-world scenarios**.\n",
    "\n",
    "Would you like me to help with **running the simulations** or **fine-tuning the Q-learning model**? Let me know how youâ€™d like to proceed!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
